[{"id":"web_search","user_id":"33c6d9ce-4866-4166-a1d3-4b2ca8c6bc64","name":"Web Search","content":"\"\"\"\ntitle: Web Search using SearXNG and Scrape first N Pages\nauthor: constLiakos with enhancements by justinh-rahb and ther3zz\nfunding_url: https://github.com/open-webui\nversion: 0.1.12\nlicense: MIT\n\"\"\"\n\nimport os\nimport requests\nfrom datetime import datetime\nimport json\nfrom requests import get\nfrom bs4 import BeautifulSoup\nimport concurrent.futures\nfrom html.parser import HTMLParser\nfrom urllib.parse import urlparse, urljoin\nimport re\nimport unicodedata\nfrom pydantic import BaseModel, Field\nimport asyncio\nfrom typing import Callable, Any\n\n\nclass HelpFunctions:\n    def __init__(self):\n        pass\n\n    def get_base_url(self, url):\n        parsed_url = urlparse(url)\n        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n        return base_url\n\n    def generate_excerpt(self, content, max_length=200):\n        return content[:max_length] + \"...\" if len(content) > max_length else content\n\n    def format_text(self, original_text):\n        soup = BeautifulSoup(original_text, \"html.parser\")\n        formatted_text = soup.get_text(separator=\" \", strip=True)\n        formatted_text = unicodedata.normalize(\"NFKC\", formatted_text)\n        formatted_text = re.sub(r\"\\s+\", \" \", formatted_text)\n        formatted_text = formatted_text.strip()\n        formatted_text = self.remove_emojis(formatted_text)\n        return formatted_text\n\n    def remove_emojis(self, text):\n        return \"\".join(c for c in text if not unicodedata.category(c).startswith(\"So\"))\n\n    def process_search_result(self, result, valves):\n        title_site = self.remove_emojis(result[\"title\"])\n        url_site = result[\"url\"]\n        snippet = result.get(\"content\", \"\")\n\n        # Check if the website is in the ignored list, but only if IGNORED_WEBSITES is not empty\n        if valves.IGNORED_WEBSITES:\n            base_url = self.get_base_url(url_site)\n            if any(\n                ignored_site.strip() in base_url\n                for ignored_site in valves.IGNORED_WEBSITES.split(\",\")\n            ):\n                return None\n\n        try:\n            response_site = requests.get(url_site, timeout=20)\n            response_site.raise_for_status()\n            html_content = response_site.text\n\n            soup = BeautifulSoup(html_content, \"html.parser\")\n            content_site = self.format_text(soup.get_text(separator=\" \", strip=True))\n\n            truncated_content = self.truncate_to_n_words(\n                content_site, valves.PAGE_CONTENT_WORDS_LIMIT\n            )\n\n            return {\n                \"title\": title_site,\n                \"url\": url_site,\n                \"content\": truncated_content,\n                \"snippet\": self.remove_emojis(snippet),\n            }\n\n        except requests.exceptions.RequestException as e:\n            return None\n\n    def truncate_to_n_words(self, text, token_limit):\n        tokens = text.split()\n        truncated_tokens = tokens[:token_limit]\n        return \" \".join(truncated_tokens)\n\n\nclass EventEmitter:\n    def __init__(self, event_emitter: Callable[[dict], Any] = None):\n        self.event_emitter = event_emitter\n\n    async def emit(self, description=\"Unknown State\", status=\"in_progress\", done=False):\n        if self.event_emitter:\n            await self.event_emitter(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": status,\n                        \"description\": description,\n                        \"done\": done,\n                    },\n                }\n            )\n\n\nclass Tools:\n    class Valves(BaseModel):\n        SEARXNG_ENGINE_API_BASE_URL: str = Field(\n            default=\"https://example.com/search\",\n            description=\"The base URL for Search Engine\",\n        )\n        IGNORED_WEBSITES: str = Field(\n            default=\"\",\n            description=\"Comma-separated list of websites to ignore\",\n        )\n        RETURNED_SCRAPPED_PAGES_NO: int = Field(\n            default=3,\n            description=\"The number of Search Engine Results to Parse\",\n        )\n        SCRAPPED_PAGES_NO: int = Field(\n            default=5,\n            description=\"Total pages scapped. Ideally greater than one of the returned pages\",\n        )\n        PAGE_CONTENT_WORDS_LIMIT: int = Field(\n            default=5000,\n            description=\"Limit words content for each page.\",\n        )\n        CITATION_LINKS: bool = Field(\n            default=False,\n            description=\"If True, send custom citations with links\",\n        )\n\n    def __init__(self):\n        self.valves = self.Valves()\n        self.headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n        }\n\n    async def search_web(\n        self,\n        query: str,\n        __event_emitter__: Callable[[dict], Any] = None,\n    ) -> str:\n        \"\"\"\n        Search the web and get the content of the relevant pages. Search for unknown knowledge, news, info, public contact info, weather, etc.\n        :params query: Web Query used in search engine.\n        :return: The content of the pages in json format.\n        \"\"\"\n        functions = HelpFunctions()\n        emitter = EventEmitter(__event_emitter__)\n\n        await emitter.emit(f\"Initiating web search for: {query}\")\n\n        search_engine_url = self.valves.SEARXNG_ENGINE_API_BASE_URL\n\n        # Ensure RETURNED_SCRAPPED_PAGES_NO does not exceed SCRAPPED_PAGES_NO\n        if self.valves.RETURNED_SCRAPPED_PAGES_NO > self.valves.SCRAPPED_PAGES_NO:\n            self.valves.RETURNED_SCRAPPED_PAGES_NO = self.valves.SCRAPPED_PAGES_NO\n\n        params = {\n            \"q\": query,\n            \"format\": \"json\",\n            \"number_of_results\": self.valves.RETURNED_SCRAPPED_PAGES_NO,\n        }\n\n        try:\n            await emitter.emit(\"Sending request to search engine\")\n            resp = requests.get(\n                search_engine_url, params=params, headers=self.headers, timeout=120\n            )\n            resp.raise_for_status()\n            data = resp.json()\n\n            results = data.get(\"results\", [])\n            limited_results = results[: self.valves.SCRAPPED_PAGES_NO]\n            await emitter.emit(f\"Retrieved {len(limited_results)} search results\")\n\n        except requests.exceptions.RequestException as e:\n            await emitter.emit(\n                status=\"error\",\n                description=f\"Error during search: {str(e)}\",\n                done=True,\n            )\n            return json.dumps({\"error\": str(e)})\n\n        results_json = []\n        if limited_results:\n            await emitter.emit(f\"Processing search results\")\n\n            with concurrent.futures.ThreadPoolExecutor() as executor:\n                futures = [\n                    executor.submit(\n                        functions.process_search_result, result, self.valves\n                    )\n                    for result in limited_results\n                ]\n                for future in concurrent.futures.as_completed(futures):\n                    result_json = future.result()\n                    if result_json:\n                        try:\n                            json.dumps(result_json)\n                            results_json.append(result_json)\n                        except (TypeError, ValueError):\n                            continue\n                    if len(results_json) >= self.valves.RETURNED_SCRAPPED_PAGES_NO:\n                        break\n\n            results_json = results_json[: self.valves.RETURNED_SCRAPPED_PAGES_NO]\n\n            if self.valves.CITATION_LINKS and __event_emitter__:\n                for result in results_json:\n                    await __event_emitter__(\n                        {\n                            \"type\": \"citation\",\n                            \"data\": {\n                                \"document\": [result[\"content\"]],\n                                \"metadata\": [{\"source\": result[\"url\"]}],\n                                \"source\": {\"name\": result[\"title\"]},\n                            },\n                        }\n                    )\n\n        await emitter.emit(\n            status=\"complete\",\n            description=f\"Web search completed. Retrieved content from {len(results_json)} pages\",\n            done=True,\n        )\n\n        return json.dumps(results_json, ensure_ascii=False)\n\n    async def get_website(\n        self, url: str, __event_emitter__: Callable[[dict], Any] = None\n    ) -> str:\n        \"\"\"\n        Web scrape the website provided and get the content of it.\n        :params url: The URL of the website.\n        :return: The content of the website in json format.\n        \"\"\"\n        functions = HelpFunctions()\n        emitter = EventEmitter(__event_emitter__)\n\n        await emitter.emit(f\"Fetching content from URL: {url}\")\n\n        results_json = []\n\n        try:\n            response_site = requests.get(url, headers=self.headers, timeout=120)\n            response_site.raise_for_status()\n            html_content = response_site.text\n\n            await emitter.emit(\"Parsing website content\")\n\n            soup = BeautifulSoup(html_content, \"html.parser\")\n\n            page_title = soup.title.string if soup.title else \"No title found\"\n            page_title = unicodedata.normalize(\"NFKC\", page_title.strip())\n            page_title = functions.remove_emojis(page_title)\n            title_site = page_title\n            url_site = url\n            content_site = functions.format_text(\n                soup.get_text(separator=\" \", strip=True)\n            )\n\n            truncated_content = functions.truncate_to_n_words(\n                content_site, self.valves.PAGE_CONTENT_WORDS_LIMIT\n            )\n\n            result_site = {\n                \"title\": title_site,\n                \"url\": url_site,\n                \"content\": truncated_content,\n                \"excerpt\": functions.generate_excerpt(content_site),\n            }\n\n            results_json.append(result_site)\n\n            if self.valves.CITATION_LINKS and __event_emitter__:\n                await __event_emitter__(\n                    {\n                        \"type\": \"citation\",\n                        \"data\": {\n                            \"document\": [truncated_content],\n                            \"metadata\": [{\"source\": url_site}],\n                            \"source\": {\"name\": title_site},\n                        },\n                    }\n                )\n\n            await emitter.emit(\n                status=\"complete\",\n                description=\"Website content retrieved and processed successfully\",\n                done=True,\n            )\n\n        except requests.exceptions.RequestException as e:\n            results_json.append(\n                {\n                    \"url\": url,\n                    \"content\": f\"Failed to retrieve the page. Error: {str(e)}\",\n                }\n            )\n\n            await emitter.emit(\n                status=\"error\",\n                description=f\"Error fetching website content: {str(e)}\",\n                done=True,\n            )\n\n        return json.dumps(results_json, ensure_ascii=False)\n","specs":[{"name":"get_website","description":"Web scrape the website provided and get the content of it.","parameters":{"properties":{"url":{"type":"string"}},"required":["url"],"type":"object"}},{"name":"search_web","description":"Search the web and get the content of the relevant pages. Search for unknown knowledge, news, info, public contact info, weather, etc.","parameters":{"properties":{"query":{"type":"string"}},"required":["query"],"type":"object"}}],"meta":{"description":"Web Search using SearXNG and Scrap first N Pages","manifest":{"title":"Web Search using SearXNG and Scrape first N Pages","author":"constLiakos with enhancements by justinh-rahb and ther3zz","funding_url":"https://github.com/open-webui","version":"0.1.12","license":"MIT"}},"access_control":null,"updated_at":1740403989,"created_at":1733406356}]